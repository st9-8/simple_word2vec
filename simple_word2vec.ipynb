{"cells":[{"metadata":{},"cell_type":"markdown","source":"# SET DATA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set of documents\ndocs=[\"the house had a tiny little mouse\",\n      \"the cat saw the mouse\",\n      \"the mouse ran away from the house\",\n      \"the cat finally ate the mouse\",\n      \"the end of the mouse story\"\n     ]\n\n# Corpus of distinct word\ncorpus = [\"the\", \"house\", \"had\", \"a\", \"tiny\", \"little\", \"mouse\", \"cat\", \"saw\",\n          \"mouse\", \"ran\", \"away\", \"away\", \"from\", \"finally\", \"ate\", \"the\", \"end\", \"of\", \"story\"]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA PREPARATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\n\n# tokenization by word\ndata = [doc.split() for doc in docs]\n\nwords_count = defaultdict(int)\nfor row in data:\n    for word in row:\n        words_count[word] += 1\n\n# number of unique words\nv_count = len(words_count.keys()) \n\n# list of unique words\nwords_list = list(words_count.keys()) \n\n# word:index\nword_index = dict((word, i) for i, word in enumerate(words_list)) \n\n#index:word\nindex_word = dict(map(lambda item: (item[1], item[0]), word_index.items())) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# COUNT VECTOR"},{"metadata":{"trusted":true},"cell_type":"code","source":"def word2countvector(data):\n    words_encoded = dict()\n    # Encode all words in our corpus\n    for word in words_list:\n        word_vec = [0 for i in range(len(data))]\n        \n        for i in range(len(data)):\n            word_vec[i] = data[i].count(word)\n\n        words_encoded[word] = word_vec\n        \n    # Encode all our dataset\n    data_countvector_encoded = []\n    for doc in data:\n        countvector_doc = []\n        for word in doc:\n            countvector_doc.append(words_encoded[word])\n        data_countvector_encoded.append(countvector_doc)\n    return words_encoded, data_countvector_encoded\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ONE HOT ENCODING"},{"metadata":{"trusted":true},"cell_type":"code","source":"def word2onehot(data):\n    \n    data_onehot_encoded = []\n    \n    for doc in data:\n        onehot_doc = []\n        for word in doc:\n            word_vec = [0 for i in range(v_count)]\n\n            index = word_index[word]\n\n            word_vec[index] = 1\n            onehot_doc.append(word_vec)\n        data_onehot_encoded.append(onehot_doc)\n    return data_onehot_encoded","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import log\ndef word2tfidf(data):\n    def presence_in_document(word):\n        presence = 0\n        for doc in data:\n            presence += 1 if doc.count(word) != 0 else 0\n        return presence\n    def replace(doc, x, y):\n        return list(map((lambda item: y if item == x else item), doc))\n    \n    data_tfidf_encoded = []\n    for doc in data:\n        tfidf_doc = doc\n        for word in doc:\n            if isinstance(word, str):\n                tf = doc.count(word)/len(doc)\n                idf = log((len(data)/presence_in_document(word)), 10)\n\n                numeric_word = tf*idf\n                tfidf_doc = replace(tfidf_doc, word, numeric_word)\n        data_tfidf_encoded.append(tfidf_doc)\n    return data_tfidf_encoded","execution_count":28,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# VECTORISATION"},{"metadata":{},"cell_type":"markdown","source":"## Vectorisation using CountVector method"},{"metadata":{"trusted":true},"cell_type":"code","source":"words_encoded, data_countvector_encoded = word2countvector(data)\ndata_countvector_encoded","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"[[[1, 2, 2, 2, 2],\n  [1, 0, 1, 0, 0],\n  [1, 0, 0, 0, 0],\n  [1, 0, 0, 0, 0],\n  [1, 0, 0, 0, 0],\n  [1, 0, 0, 0, 0],\n  [1, 1, 1, 1, 1]],\n [[1, 2, 2, 2, 2],\n  [0, 1, 0, 1, 0],\n  [0, 1, 0, 0, 0],\n  [1, 2, 2, 2, 2],\n  [1, 1, 1, 1, 1]],\n [[1, 2, 2, 2, 2],\n  [1, 1, 1, 1, 1],\n  [0, 0, 1, 0, 0],\n  [0, 0, 1, 0, 0],\n  [0, 0, 1, 0, 0],\n  [1, 2, 2, 2, 2],\n  [1, 0, 1, 0, 0]],\n [[1, 2, 2, 2, 2],\n  [0, 1, 0, 1, 0],\n  [0, 0, 0, 1, 0],\n  [0, 0, 0, 1, 0],\n  [1, 2, 2, 2, 2],\n  [1, 1, 1, 1, 1]],\n [[1, 2, 2, 2, 2],\n  [0, 0, 0, 0, 1],\n  [0, 0, 0, 0, 1],\n  [1, 2, 2, 2, 2],\n  [1, 1, 1, 1, 1],\n  [0, 0, 0, 0, 1]]]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Vectorisation using One Hot Encoding method"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_onehot_encoded = word2onehot(data)\ndata_onehot_encoded","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"[[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Vectorisation using TF-IDF Method"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_tfidf_encoded = word2tfidf(data)\ndata_tfidf_encoded","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"[[0.0,\n  0.05684857266743394,\n  0.09985285776228839,\n  0.09985285776228839,\n  0.09985285776228839,\n  0.09985285776228839,\n  0.0],\n [0.0, 0.07958800173440753, 0.13979400086720375, 0.0, 0.0],\n [0.0,\n  0.0,\n  0.09985285776228839,\n  0.09985285776228839,\n  0.09985285776228839,\n  0.0,\n  0.05684857266743394],\n [0.0,\n  0.06632333477867293,\n  0.11649500072266979,\n  0.11649500072266979,\n  0.0,\n  0.0],\n [0.0,\n  0.11649500072266979,\n  0.11649500072266979,\n  0.0,\n  0.0,\n  0.11649500072266979]]"},"metadata":{}}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}